{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmPiP1Lly21U"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.mkdir('/content/sample_data/llms')\n",
        "os.chdir('/content/sample_data/llms')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8psK2wMUykPo",
        "outputId": "06bf6cd6-4381-4f41-a0e1-ba4d10744e71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 4485, done.\u001b[K\n",
            "remote: Counting objects: 100% (2076/2076), done.\u001b[K\n",
            "remote: Compressing objects: 100% (359/359), done.\u001b[K\n",
            "remote: Total 4485 (delta 1905), reused 1778 (delta 1717), pack-reused 2409\u001b[K\n",
            "Receiving objects: 100% (4485/4485), 3.85 MiB | 10.56 MiB/s, done.\n",
            "Resolving deltas: 100% (3042/3042), done.\n",
            "/content/sample_data/llms/llama.cpp\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "%cd llama.cpp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X12brMFI0d39",
        "outputId": "3084c84d-861a-4f94-db0a-e4af72dacb7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I llama.cpp build info: \n",
            "I UNAME_S:  Linux\n",
            "I UNAME_P:  x86_64\n",
            "I UNAME_M:  x86_64\n",
            "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
            "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
            "I LDFLAGS:  \n",
            "I CC:       cc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "I CXX:      g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "\n",
            "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS   -c ggml.c -o ggml.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -c llama.cpp -o llama.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -c examples/common.cpp -o common.o\n",
            "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS   -c -o k_quants.o k_quants.c\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/main/main.cpp ggml.o llama.o common.o k_quants.o -o main \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize/quantize.cpp ggml.o llama.o k_quants.o -o quantize \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o -o quantize-stats \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o -o perplexity \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o -o embedding \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS pocs/vdot/vdot.cpp ggml.o k_quants.o -o vdot \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o -o train-text-from-scratch \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o -o simple \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o k_quants.o -o server \n",
            "g++ --shared -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o -o libembdinput.so \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o -o embd-input-test  -L. -lembdinput\n"
          ]
        }
      ],
      "source": [
        "#!mkdir build\n",
        "#%cd build\n",
        "#!cmake ..\n",
        "#!cmake --build . --config Release\n",
        "\n",
        "!make"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pO8zF4UVi2Jw",
        "outputId": "6422163f-d85b-4cbf-f9b6-3b636d9a4d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/sample_data/llms/llama.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWtBnlQrc_IH",
        "outputId": "f20f1206-56a9-4682-ffcd-3b206fcb64e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated git hooks.\n",
            "Git LFS initialized.\n",
            "Cloning into 'open_llama_3b'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 19 (delta 3), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (19/19), 6.42 KiB | 939.00 KiB/s, done.\n",
            "Encountered 1 file(s) that may not have been copied correctly on Windows:\n",
            "\tpytorch_model.bin\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ],
      "source": [
        "# Make sure you have git-lfs installed (https://git-lfs.com)\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/openlm-research/open_llama_3b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dehtSLFCH8x0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5382f925-cbf2-468f-d3dc-fbc9acb87c17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.24 (from -r requirements.txt (line 1))\n",
            "  Downloading numpy-1.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece==0.1.98 (from -r requirements.txt (line 2))\n",
            "  Downloading sentencepiece-0.1.98-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece, numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.0 which is incompatible.\n",
            "seaborn 0.12.2 requires numpy!=1.24.0,>=1.17, but you have numpy 1.24.0 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.0 sentencepiece-0.1.98\n",
            "Loading model file models/open_llama_3b/pytorch_model.bin\n",
            "Loading vocab file models/open_llama_3b/tokenizer.model\n",
            "params: n_vocab:32000 n_embd:3200 n_mult:240 n_head:32 n_layer:26\n",
            "Writing vocab...\n",
            "[  1/237] Writing tensor tok_embeddings.weight                  | size  32000 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[  2/237] Writing tensor norm.weight                            | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[  3/237] Writing tensor output.weight                          | size  32000 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[  4/237] Writing tensor layers.0.attention.wq.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[  5/237] Writing tensor layers.0.attention.wk.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[  6/237] Writing tensor layers.0.attention.wv.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[  7/237] Writing tensor layers.0.attention.wo.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[  8/237] Writing tensor layers.0.attention_norm.weight         | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[  9/237] Writing tensor layers.0.feed_forward.w1.weight        | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 10/237] Writing tensor layers.0.feed_forward.w2.weight        | size   3200 x   8640  | type UnquantizedDataType(name='F16')\n",
            "[ 11/237] Writing tensor layers.0.feed_forward.w3.weight        | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 12/237] Writing tensor layers.0.ffn_norm.weight               | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[ 13/237] Writing tensor layers.1.attention.wq.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 14/237] Writing tensor layers.1.attention.wk.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 15/237] Writing tensor layers.1.attention.wv.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 16/237] Writing tensor layers.1.attention.wo.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 17/237] Writing tensor layers.1.attention_norm.weight         | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[ 18/237] Writing tensor layers.1.feed_forward.w1.weight        | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 19/237] Writing tensor layers.1.feed_forward.w2.weight        | size   3200 x   8640  | type UnquantizedDataType(name='F16')\n",
            "[ 20/237] Writing tensor layers.1.feed_forward.w3.weight        | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 21/237] Writing tensor layers.1.ffn_norm.weight               | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[ 22/237] Writing tensor layers.2.attention.wq.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 23/237] Writing tensor layers.2.attention.wk.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 24/237] Writing tensor layers.2.attention.wv.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 25/237] Writing tensor layers.2.attention.wo.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 26/237] Writing tensor layers.2.attention_norm.weight         | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[ 27/237] Writing tensor layers.2.feed_forward.w1.weight        | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 28/237] Writing tensor layers.2.feed_forward.w2.weight        | size   3200 x   8640  | type UnquantizedDataType(name='F16')\n",
            "[ 29/237] Writing tensor layers.2.feed_forward.w3.weight        | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 30/237] Writing tensor layers.2.ffn_norm.weight               | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[ 31/237] Writing tensor layers.3.attention.wq.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 32/237] Writing tensor layers.3.attention.wk.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 33/237] Writing tensor layers.3.attention.wv.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 34/237] Writing tensor layers.3.attention.wo.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 35/237] Writing tensor layers.3.attention_norm.weight         | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[ 36/237] Writing tensor layers.3.feed_forward.w1.weight        | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 37/237] Writing tensor layers.3.feed_forward.w2.weight        | size   3200 x   8640  | type UnquantizedDataType(name='F16')\n",
            "[ 38/237] Writing tensor layers.3.feed_forward.w3.weight        | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 39/237] Writing tensor layers.3.ffn_norm.weight               | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[ 40/237] Writing tensor layers.4.attention.wq.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 41/237] Writing tensor layers.4.attention.wk.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 42/237] Writing tensor layers.4.attention.wv.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 43/237] Writing tensor layers.4.attention.wo.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 44/237] Writing tensor layers.4.attention_norm.weight         | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[ 45/237] Writing tensor layers.4.feed_forward.w1.weight        | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 46/237] Writing tensor layers.4.feed_forward.w2.weight        | size   3200 x   8640  | type UnquantizedDataType(name='F16')\n",
            "[ 47/237] Writing tensor layers.4.feed_forward.w3.weight        | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 48/237] Writing tensor layers.4.ffn_norm.weight               | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[ 49/237] Writing tensor layers.5.attention.wq.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 50/237] Writing tensor layers.5.attention.wk.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 51/237] Writing tensor layers.5.attention.wv.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 52/237] Writing tensor layers.5.attention.wo.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 53/237] Writing tensor layers.5.attention_norm.weight         | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[ 54/237] Writing tensor layers.5.feed_forward.w1.weight        | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 55/237] Writing tensor layers.5.feed_forward.w2.weight        | size   3200 x   8640  | type UnquantizedDataType(name='F16')\n",
            "[ 56/237] Writing tensor layers.5.feed_forward.w3.weight        | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 57/237] Writing tensor layers.5.ffn_norm.weight               | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[ 58/237] Writing tensor layers.6.attention.wq.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 59/237] Writing tensor layers.6.attention.wk.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 60/237] Writing tensor layers.6.attention.wv.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 61/237] Writing tensor layers.6.attention.wo.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 62/237] Writing tensor layers.6.attention_norm.weight         | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[ 63/237] Writing tensor layers.6.feed_forward.w1.weight        | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 64/237] Writing tensor layers.6.feed_forward.w2.weight        | size   3200 x   8640  | type UnquantizedDataType(name='F16')\n",
            "[ 65/237] Writing tensor layers.6.feed_forward.w3.weight        | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 66/237] Writing tensor layers.6.ffn_norm.weight               | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[ 67/237] Writing tensor layers.7.attention.wq.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 68/237] Writing tensor layers.7.attention.wk.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 69/237] Writing tensor layers.7.attention.wv.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 70/237] Writing tensor layers.7.attention.wo.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 71/237] Writing tensor layers.7.attention_norm.weight         | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[ 72/237] Writing tensor layers.7.feed_forward.w1.weight        | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 73/237] Writing tensor layers.7.feed_forward.w2.weight        | size   3200 x   8640  | type UnquantizedDataType(name='F16')\n",
            "[ 74/237] Writing tensor layers.7.feed_forward.w3.weight        | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 75/237] Writing tensor layers.7.ffn_norm.weight               | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[ 76/237] Writing tensor layers.8.attention.wq.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 77/237] Writing tensor layers.8.attention.wk.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 78/237] Writing tensor layers.8.attention.wv.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 79/237] Writing tensor layers.8.attention.wo.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 80/237] Writing tensor layers.8.attention_norm.weight         | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[ 81/237] Writing tensor layers.8.feed_forward.w1.weight        | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 82/237] Writing tensor layers.8.feed_forward.w2.weight        | size   3200 x   8640  | type UnquantizedDataType(name='F16')\n",
            "[ 83/237] Writing tensor layers.8.feed_forward.w3.weight        | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 84/237] Writing tensor layers.8.ffn_norm.weight               | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[ 85/237] Writing tensor layers.9.attention.wq.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 86/237] Writing tensor layers.9.attention.wk.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 87/237] Writing tensor layers.9.attention.wv.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 88/237] Writing tensor layers.9.attention.wo.weight           | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 89/237] Writing tensor layers.9.attention_norm.weight         | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[ 90/237] Writing tensor layers.9.feed_forward.w1.weight        | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 91/237] Writing tensor layers.9.feed_forward.w2.weight        | size   3200 x   8640  | type UnquantizedDataType(name='F16')\n",
            "[ 92/237] Writing tensor layers.9.feed_forward.w3.weight        | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 93/237] Writing tensor layers.9.ffn_norm.weight               | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[ 94/237] Writing tensor layers.10.attention.wq.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 95/237] Writing tensor layers.10.attention.wk.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 96/237] Writing tensor layers.10.attention.wv.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 97/237] Writing tensor layers.10.attention.wo.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[ 98/237] Writing tensor layers.10.attention_norm.weight        | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[ 99/237] Writing tensor layers.10.feed_forward.w1.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[100/237] Writing tensor layers.10.feed_forward.w2.weight       | size   3200 x   8640  | type UnquantizedDataType(name='F16')\n",
            "[101/237] Writing tensor layers.10.feed_forward.w3.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[102/237] Writing tensor layers.10.ffn_norm.weight              | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[103/237] Writing tensor layers.11.attention.wq.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[104/237] Writing tensor layers.11.attention.wk.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[105/237] Writing tensor layers.11.attention.wv.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[106/237] Writing tensor layers.11.attention.wo.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[107/237] Writing tensor layers.11.attention_norm.weight        | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[108/237] Writing tensor layers.11.feed_forward.w1.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[109/237] Writing tensor layers.11.feed_forward.w2.weight       | size   3200 x   8640  | type UnquantizedDataType(name='F16')\n",
            "[110/237] Writing tensor layers.11.feed_forward.w3.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[111/237] Writing tensor layers.11.ffn_norm.weight              | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[112/237] Writing tensor layers.12.attention.wq.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[113/237] Writing tensor layers.12.attention.wk.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[114/237] Writing tensor layers.12.attention.wv.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[115/237] Writing tensor layers.12.attention.wo.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[116/237] Writing tensor layers.12.attention_norm.weight        | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[117/237] Writing tensor layers.12.feed_forward.w1.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[118/237] Writing tensor layers.12.feed_forward.w2.weight       | size   3200 x   8640  | type UnquantizedDataType(name='F16')\n",
            "[119/237] Writing tensor layers.12.feed_forward.w3.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[120/237] Writing tensor layers.12.ffn_norm.weight              | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[121/237] Writing tensor layers.13.attention.wq.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[122/237] Writing tensor layers.13.attention.wk.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[123/237] Writing tensor layers.13.attention.wv.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[124/237] Writing tensor layers.13.attention.wo.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[125/237] Writing tensor layers.13.attention_norm.weight        | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[126/237] Writing tensor layers.13.feed_forward.w1.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[127/237] Writing tensor layers.13.feed_forward.w2.weight       | size   3200 x   8640  | type UnquantizedDataType(name='F16')\n",
            "[128/237] Writing tensor layers.13.feed_forward.w3.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[129/237] Writing tensor layers.13.ffn_norm.weight              | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[130/237] Writing tensor layers.14.attention.wq.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[131/237] Writing tensor layers.14.attention.wk.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[132/237] Writing tensor layers.14.attention.wv.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[133/237] Writing tensor layers.14.attention.wo.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[134/237] Writing tensor layers.14.attention_norm.weight        | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[135/237] Writing tensor layers.14.feed_forward.w1.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[136/237] Writing tensor layers.14.feed_forward.w2.weight       | size   3200 x   8640  | type UnquantizedDataType(name='F16')\n",
            "[137/237] Writing tensor layers.14.feed_forward.w3.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[138/237] Writing tensor layers.14.ffn_norm.weight              | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[139/237] Writing tensor layers.15.attention.wq.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[140/237] Writing tensor layers.15.attention.wk.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[141/237] Writing tensor layers.15.attention.wv.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[142/237] Writing tensor layers.15.attention.wo.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[143/237] Writing tensor layers.15.attention_norm.weight        | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[144/237] Writing tensor layers.15.feed_forward.w1.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[145/237] Writing tensor layers.15.feed_forward.w2.weight       | size   3200 x   8640  | type UnquantizedDataType(name='F16')\n",
            "[146/237] Writing tensor layers.15.feed_forward.w3.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[147/237] Writing tensor layers.15.ffn_norm.weight              | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[148/237] Writing tensor layers.16.attention.wq.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[149/237] Writing tensor layers.16.attention.wk.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[150/237] Writing tensor layers.16.attention.wv.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[151/237] Writing tensor layers.16.attention.wo.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[152/237] Writing tensor layers.16.attention_norm.weight        | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[153/237] Writing tensor layers.16.feed_forward.w1.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[154/237] Writing tensor layers.16.feed_forward.w2.weight       | size   3200 x   8640  | type UnquantizedDataType(name='F16')\n",
            "[155/237] Writing tensor layers.16.feed_forward.w3.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[156/237] Writing tensor layers.16.ffn_norm.weight              | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[157/237] Writing tensor layers.17.attention.wq.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[158/237] Writing tensor layers.17.attention.wk.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[159/237] Writing tensor layers.17.attention.wv.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[160/237] Writing tensor layers.17.attention.wo.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[161/237] Writing tensor layers.17.attention_norm.weight        | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[162/237] Writing tensor layers.17.feed_forward.w1.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[163/237] Writing tensor layers.17.feed_forward.w2.weight       | size   3200 x   8640  | type UnquantizedDataType(name='F16')\n",
            "[164/237] Writing tensor layers.17.feed_forward.w3.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[165/237] Writing tensor layers.17.ffn_norm.weight              | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[166/237] Writing tensor layers.18.attention.wq.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[167/237] Writing tensor layers.18.attention.wk.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[168/237] Writing tensor layers.18.attention.wv.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[169/237] Writing tensor layers.18.attention.wo.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[170/237] Writing tensor layers.18.attention_norm.weight        | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[171/237] Writing tensor layers.18.feed_forward.w1.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[172/237] Writing tensor layers.18.feed_forward.w2.weight       | size   3200 x   8640  | type UnquantizedDataType(name='F16')\n",
            "[173/237] Writing tensor layers.18.feed_forward.w3.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[174/237] Writing tensor layers.18.ffn_norm.weight              | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[175/237] Writing tensor layers.19.attention.wq.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[176/237] Writing tensor layers.19.attention.wk.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[177/237] Writing tensor layers.19.attention.wv.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[178/237] Writing tensor layers.19.attention.wo.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[179/237] Writing tensor layers.19.attention_norm.weight        | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[180/237] Writing tensor layers.19.feed_forward.w1.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[181/237] Writing tensor layers.19.feed_forward.w2.weight       | size   3200 x   8640  | type UnquantizedDataType(name='F16')\n",
            "[182/237] Writing tensor layers.19.feed_forward.w3.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[183/237] Writing tensor layers.19.ffn_norm.weight              | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[184/237] Writing tensor layers.20.attention.wq.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[185/237] Writing tensor layers.20.attention.wk.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[186/237] Writing tensor layers.20.attention.wv.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[187/237] Writing tensor layers.20.attention.wo.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[188/237] Writing tensor layers.20.attention_norm.weight        | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[189/237] Writing tensor layers.20.feed_forward.w1.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[190/237] Writing tensor layers.20.feed_forward.w2.weight       | size   3200 x   8640  | type UnquantizedDataType(name='F16')\n",
            "[191/237] Writing tensor layers.20.feed_forward.w3.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[192/237] Writing tensor layers.20.ffn_norm.weight              | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[193/237] Writing tensor layers.21.attention.wq.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[194/237] Writing tensor layers.21.attention.wk.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[195/237] Writing tensor layers.21.attention.wv.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[196/237] Writing tensor layers.21.attention.wo.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[197/237] Writing tensor layers.21.attention_norm.weight        | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[198/237] Writing tensor layers.21.feed_forward.w1.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[199/237] Writing tensor layers.21.feed_forward.w2.weight       | size   3200 x   8640  | type UnquantizedDataType(name='F16')\n",
            "[200/237] Writing tensor layers.21.feed_forward.w3.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[201/237] Writing tensor layers.21.ffn_norm.weight              | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[202/237] Writing tensor layers.22.attention.wq.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[203/237] Writing tensor layers.22.attention.wk.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[204/237] Writing tensor layers.22.attention.wv.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[205/237] Writing tensor layers.22.attention.wo.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[206/237] Writing tensor layers.22.attention_norm.weight        | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[207/237] Writing tensor layers.22.feed_forward.w1.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[208/237] Writing tensor layers.22.feed_forward.w2.weight       | size   3200 x   8640  | type UnquantizedDataType(name='F16')\n",
            "[209/237] Writing tensor layers.22.feed_forward.w3.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[210/237] Writing tensor layers.22.ffn_norm.weight              | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[211/237] Writing tensor layers.23.attention.wq.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[212/237] Writing tensor layers.23.attention.wk.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[213/237] Writing tensor layers.23.attention.wv.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[214/237] Writing tensor layers.23.attention.wo.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[215/237] Writing tensor layers.23.attention_norm.weight        | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[216/237] Writing tensor layers.23.feed_forward.w1.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[217/237] Writing tensor layers.23.feed_forward.w2.weight       | size   3200 x   8640  | type UnquantizedDataType(name='F16')\n",
            "[218/237] Writing tensor layers.23.feed_forward.w3.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[219/237] Writing tensor layers.23.ffn_norm.weight              | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[220/237] Writing tensor layers.24.attention.wq.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[221/237] Writing tensor layers.24.attention.wk.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[222/237] Writing tensor layers.24.attention.wv.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[223/237] Writing tensor layers.24.attention.wo.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[224/237] Writing tensor layers.24.attention_norm.weight        | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[225/237] Writing tensor layers.24.feed_forward.w1.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[226/237] Writing tensor layers.24.feed_forward.w2.weight       | size   3200 x   8640  | type UnquantizedDataType(name='F16')\n",
            "[227/237] Writing tensor layers.24.feed_forward.w3.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[228/237] Writing tensor layers.24.ffn_norm.weight              | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[229/237] Writing tensor layers.25.attention.wq.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[230/237] Writing tensor layers.25.attention.wk.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[231/237] Writing tensor layers.25.attention.wv.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[232/237] Writing tensor layers.25.attention.wo.weight          | size   3200 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[233/237] Writing tensor layers.25.attention_norm.weight        | size   3200           | type UnquantizedDataType(name='F32')\n",
            "[234/237] Writing tensor layers.25.feed_forward.w1.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[235/237] Writing tensor layers.25.feed_forward.w2.weight       | size   3200 x   8640  | type UnquantizedDataType(name='F16')\n",
            "[236/237] Writing tensor layers.25.feed_forward.w3.weight       | size   8640 x   3200  | type UnquantizedDataType(name='F16')\n",
            "[237/237] Writing tensor layers.25.ffn_norm.weight              | size   3200           | type UnquantizedDataType(name='F32')\n",
            "Wrote models/open_llama_3b/ggml-model-f16.bin\n"
          ]
        }
      ],
      "source": [
        "# obtain the original LLaMA model weights and place them in ./models\n",
        "import shutil\n",
        "shutil.move(\"./open_llama_3b\", \"./models/open_llama_3b\")\n",
        "\n",
        "# install Python dependencies\n",
        "!python3 -m pip install -r requirements.txt\n",
        "\n",
        "# convert the 7B model to ggml FP16 format\n",
        "!python3 convert.py models/open_llama_3b"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDAz_iV-eH-J",
        "outputId": "2c56f47f-0cc8-4576-bdf4-ff8b56cf59bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build-info.h\t\t ggml-metal.m\t   Package.swift\n",
            "build.zig\t\t ggml-metal.metal  perplexity\n",
            "CMakeLists.txt\t\t ggml.o\t\t   pocs\n",
            "common.o\t\t ggml-opencl.cpp   prompts\n",
            "convert-lora-to-ggml.py  ggml-opencl.h\t   quantize\n",
            "convert-pth-to-ggml.py\t k_quants.c\t   quantize-stats\n",
            "convert.py\t\t k_quants.h\t   README.md\n",
            "docs\t\t\t k_quants.o\t   requirements.txt\n",
            "embd-input-test\t\t libembdinput.so   scripts\n",
            "embedding\t\t LICENSE\t   server\n",
            "examples\t\t llama.cpp\t   SHA256SUMS\n",
            "flake.lock\t\t llama.h\t   simple\n",
            "flake.nix\t\t llama.o\t   spm-headers\n",
            "ggml.c\t\t\t llama-util.h\t   tests\n",
            "ggml-cuda.cu\t\t main\t\t   train-text-from-scratch\n",
            "ggml-cuda.h\t\t Makefile\t   vdot\n",
            "ggml.h\t\t\t media\n",
            "ggml-metal.h\t\t models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# quantize the model to 4-bits (using q4_0 method)\n",
        "!./quantize ./models/open_llama_3b/ggml-model-f16.bin ./models/open_llama_3b/ggml-model-q4_0.bin q4_0\n",
        "\n",
        "# run the inference\n",
        "!./main -m ./models/open_llama_3b/ggml-model-q4_0.bin -n 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EoCOw9cdbN9",
        "outputId": "3107cd97-4d5d-4a07-9191-5db6be60c8d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 800 (481f793)\n",
            "main: quantizing './models/open_llama_3b/ggml-model-f16.bin' to './models/open_llama_3b/ggml-model-q4_0.bin' as Q4_0\n",
            "llama.cpp: loading model from ./models/open_llama_3b/ggml-model-f16.bin\n",
            "llama.cpp: saving model to ./models/open_llama_3b/ggml-model-q4_0.bin\n",
            "[   1/ 237]                tok_embeddings.weight -     3200 x 32000, type =    f16, quantizing .. size =   195.31 MB ->    54.93 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[   2/ 237]                          norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[   3/ 237]                        output.weight -     3200 x 32000, type =    f16, quantizing .. size =   195.31 MB ->    54.93 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
            "[   4/ 237]         layers.0.attention.wq.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.014 0.023 0.035 0.053 0.074 0.097 0.118 0.130 0.118 0.097 0.074 0.053 0.036 0.023 0.019 \n",
            "[   5/ 237]         layers.0.attention.wk.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.013 0.021 0.034 0.051 0.073 0.098 0.122 0.134 0.122 0.098 0.073 0.051 0.034 0.021 0.018 \n",
            "[   6/ 237]         layers.0.attention.wv.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.075 0.096 0.115 0.124 0.115 0.097 0.075 0.055 0.037 0.024 0.020 \n",
            "[   7/ 237]         layers.0.attention.wo.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.024 0.037 0.054 0.076 0.098 0.115 0.122 0.115 0.098 0.076 0.054 0.037 0.024 0.019 \n",
            "[   8/ 237]       layers.0.attention_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[   9/ 237]      layers.0.feed_forward.w1.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.026 0.040 0.057 0.077 0.096 0.110 0.115 0.110 0.096 0.077 0.057 0.040 0.026 0.021 \n",
            "[  10/ 237]      layers.0.feed_forward.w2.weight -     8640 x  3200, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "[  11/ 237]      layers.0.feed_forward.w3.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.026 0.039 0.057 0.077 0.096 0.111 0.116 0.110 0.096 0.077 0.057 0.039 0.026 0.021 \n",
            "[  12/ 237]             layers.0.ffn_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[  13/ 237]         layers.1.attention.wq.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "[  14/ 237]         layers.1.attention.wk.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.113 0.119 0.113 0.097 0.076 0.056 0.038 0.025 0.020 \n",
            "[  15/ 237]         layers.1.attention.wv.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.037 0.024 0.020 \n",
            "[  16/ 237]         layers.1.attention.wo.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.024 0.038 0.055 0.076 0.097 0.114 0.120 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
            "[  17/ 237]       layers.1.attention_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[  18/ 237]      layers.1.feed_forward.w1.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[  19/ 237]      layers.1.feed_forward.w2.weight -     8640 x  3200, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[  20/ 237]      layers.1.feed_forward.w3.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[  21/ 237]             layers.1.ffn_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[  22/ 237]         layers.2.attention.wq.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
            "[  23/ 237]         layers.2.attention.wk.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
            "[  24/ 237]         layers.2.attention.wv.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.111 0.119 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
            "[  25/ 237]         layers.2.attention.wo.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.038 0.025 0.021 \n",
            "[  26/ 237]       layers.2.attention_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[  27/ 237]      layers.2.feed_forward.w1.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[  28/ 237]      layers.2.feed_forward.w2.weight -     8640 x  3200, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[  29/ 237]      layers.2.feed_forward.w3.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[  30/ 237]             layers.2.ffn_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[  31/ 237]         layers.3.attention.wq.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
            "[  32/ 237]         layers.3.attention.wk.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.038 0.025 0.021 \n",
            "[  33/ 237]         layers.3.attention.wv.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
            "[  34/ 237]         layers.3.attention.wo.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "[  35/ 237]       layers.3.attention_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[  36/ 237]      layers.3.feed_forward.w1.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[  37/ 237]      layers.3.feed_forward.w2.weight -     8640 x  3200, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[  38/ 237]      layers.3.feed_forward.w3.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[  39/ 237]             layers.3.ffn_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[  40/ 237]         layers.4.attention.wq.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
            "[  41/ 237]         layers.4.attention.wk.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
            "[  42/ 237]         layers.4.attention.wv.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
            "[  43/ 237]         layers.4.attention.wo.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[  44/ 237]       layers.4.attention_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[  45/ 237]      layers.4.feed_forward.w1.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[  46/ 237]      layers.4.feed_forward.w2.weight -     8640 x  3200, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[  47/ 237]      layers.4.feed_forward.w3.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[  48/ 237]             layers.4.ffn_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[  49/ 237]         layers.5.attention.wq.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
            "[  50/ 237]         layers.5.attention.wk.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "[  51/ 237]         layers.5.attention.wv.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
            "[  52/ 237]         layers.5.attention.wo.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "[  53/ 237]       layers.5.attention_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[  54/ 237]      layers.5.feed_forward.w1.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[  55/ 237]      layers.5.feed_forward.w2.weight -     8640 x  3200, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[  56/ 237]      layers.5.feed_forward.w3.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[  57/ 237]             layers.5.ffn_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[  58/ 237]         layers.6.attention.wq.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "[  59/ 237]         layers.6.attention.wk.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
            "[  60/ 237]         layers.6.attention.wv.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.119 0.111 0.096 0.076 0.056 0.039 0.025 0.021 \n",
            "[  61/ 237]         layers.6.attention.wo.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "[  62/ 237]       layers.6.attention_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[  63/ 237]      layers.6.feed_forward.w1.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[  64/ 237]      layers.6.feed_forward.w2.weight -     8640 x  3200, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[  65/ 237]      layers.6.feed_forward.w3.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[  66/ 237]             layers.6.ffn_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[  67/ 237]         layers.7.attention.wq.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.097 0.111 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
            "[  68/ 237]         layers.7.attention.wk.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
            "[  69/ 237]         layers.7.attention.wv.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.119 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
            "[  70/ 237]         layers.7.attention.wo.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "[  71/ 237]       layers.7.attention_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[  72/ 237]      layers.7.feed_forward.w1.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[  73/ 237]      layers.7.feed_forward.w2.weight -     8640 x  3200, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "[  74/ 237]      layers.7.feed_forward.w3.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[  75/ 237]             layers.7.ffn_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[  76/ 237]         layers.8.attention.wq.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
            "[  77/ 237]         layers.8.attention.wk.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.112 0.097 0.076 0.056 0.039 0.025 0.021 \n",
            "[  78/ 237]         layers.8.attention.wv.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
            "[  79/ 237]         layers.8.attention.wo.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[  80/ 237]       layers.8.attention_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[  81/ 237]      layers.8.feed_forward.w1.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[  82/ 237]      layers.8.feed_forward.w2.weight -     8640 x  3200, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "[  83/ 237]      layers.8.feed_forward.w3.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[  84/ 237]             layers.8.ffn_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[  85/ 237]         layers.9.attention.wq.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
            "[  86/ 237]         layers.9.attention.wk.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
            "[  87/ 237]         layers.9.attention.wv.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.111 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
            "[  88/ 237]         layers.9.attention.wo.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "[  89/ 237]       layers.9.attention_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[  90/ 237]      layers.9.feed_forward.w1.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[  91/ 237]      layers.9.feed_forward.w2.weight -     8640 x  3200, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "[  92/ 237]      layers.9.feed_forward.w3.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[  93/ 237]             layers.9.ffn_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[  94/ 237]        layers.10.attention.wq.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "[  95/ 237]        layers.10.attention.wk.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[  96/ 237]        layers.10.attention.wv.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
            "[  97/ 237]        layers.10.attention.wo.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "[  98/ 237]      layers.10.attention_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[  99/ 237]     layers.10.feed_forward.w1.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 100/ 237]     layers.10.feed_forward.w2.weight -     8640 x  3200, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 101/ 237]     layers.10.feed_forward.w3.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 102/ 237]            layers.10.ffn_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 103/ 237]        layers.11.attention.wq.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 104/ 237]        layers.11.attention.wk.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 105/ 237]        layers.11.attention.wv.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
            "[ 106/ 237]        layers.11.attention.wo.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 107/ 237]      layers.11.attention_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 108/ 237]     layers.11.feed_forward.w1.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 109/ 237]     layers.11.feed_forward.w2.weight -     8640 x  3200, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 110/ 237]     layers.11.feed_forward.w3.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 111/ 237]            layers.11.ffn_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 112/ 237]        layers.12.attention.wq.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 113/ 237]        layers.12.attention.wk.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 114/ 237]        layers.12.attention.wv.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
            "[ 115/ 237]        layers.12.attention.wo.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 116/ 237]      layers.12.attention_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 117/ 237]     layers.12.feed_forward.w1.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 118/ 237]     layers.12.feed_forward.w2.weight -     8640 x  3200, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 119/ 237]     layers.12.feed_forward.w3.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 120/ 237]            layers.12.ffn_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 121/ 237]        layers.13.attention.wq.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 122/ 237]        layers.13.attention.wk.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 123/ 237]        layers.13.attention.wv.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
            "[ 124/ 237]        layers.13.attention.wo.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 125/ 237]      layers.13.attention_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 126/ 237]     layers.13.feed_forward.w1.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 127/ 237]     layers.13.feed_forward.w2.weight -     8640 x  3200, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 128/ 237]     layers.13.feed_forward.w3.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 129/ 237]            layers.13.ffn_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 130/ 237]        layers.14.attention.wq.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 131/ 237]        layers.14.attention.wk.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 132/ 237]        layers.14.attention.wv.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
            "[ 133/ 237]        layers.14.attention.wo.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 134/ 237]      layers.14.attention_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 135/ 237]     layers.14.feed_forward.w1.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 136/ 237]     layers.14.feed_forward.w2.weight -     8640 x  3200, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 137/ 237]     layers.14.feed_forward.w3.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 138/ 237]            layers.14.ffn_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 139/ 237]        layers.15.attention.wq.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 140/ 237]        layers.15.attention.wk.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 141/ 237]        layers.15.attention.wv.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
            "[ 142/ 237]        layers.15.attention.wo.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 143/ 237]      layers.15.attention_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 144/ 237]     layers.15.feed_forward.w1.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 145/ 237]     layers.15.feed_forward.w2.weight -     8640 x  3200, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 146/ 237]     layers.15.feed_forward.w3.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 147/ 237]            layers.15.ffn_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 148/ 237]        layers.16.attention.wq.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 149/ 237]        layers.16.attention.wk.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 150/ 237]        layers.16.attention.wv.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
            "[ 151/ 237]        layers.16.attention.wo.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 152/ 237]      layers.16.attention_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 153/ 237]     layers.16.feed_forward.w1.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 154/ 237]     layers.16.feed_forward.w2.weight -     8640 x  3200, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 155/ 237]     layers.16.feed_forward.w3.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 156/ 237]            layers.16.ffn_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 157/ 237]        layers.17.attention.wq.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 158/ 237]        layers.17.attention.wk.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 159/ 237]        layers.17.attention.wv.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.119 0.112 0.096 0.076 0.056 0.039 0.025 0.021 \n",
            "[ 160/ 237]        layers.17.attention.wo.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 161/ 237]      layers.17.attention_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 162/ 237]     layers.17.feed_forward.w1.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 163/ 237]     layers.17.feed_forward.w2.weight -     8640 x  3200, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 164/ 237]     layers.17.feed_forward.w3.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 165/ 237]            layers.17.ffn_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 166/ 237]        layers.18.attention.wq.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 167/ 237]        layers.18.attention.wk.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 168/ 237]        layers.18.attention.wv.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 169/ 237]        layers.18.attention.wo.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 170/ 237]      layers.18.attention_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 171/ 237]     layers.18.feed_forward.w1.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 172/ 237]     layers.18.feed_forward.w2.weight -     8640 x  3200, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 173/ 237]     layers.18.feed_forward.w3.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 174/ 237]            layers.18.ffn_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 175/ 237]        layers.19.attention.wq.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.096 0.112 0.118 0.112 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 176/ 237]        layers.19.attention.wk.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 177/ 237]        layers.19.attention.wv.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 178/ 237]        layers.19.attention.wo.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 179/ 237]      layers.19.attention_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 180/ 237]     layers.19.feed_forward.w1.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 181/ 237]     layers.19.feed_forward.w2.weight -     8640 x  3200, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 182/ 237]     layers.19.feed_forward.w3.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 183/ 237]            layers.19.ffn_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 184/ 237]        layers.20.attention.wq.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 185/ 237]        layers.20.attention.wk.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 186/ 237]        layers.20.attention.wv.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 187/ 237]        layers.20.attention.wo.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 188/ 237]      layers.20.attention_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 189/ 237]     layers.20.feed_forward.w1.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 190/ 237]     layers.20.feed_forward.w2.weight -     8640 x  3200, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 191/ 237]     layers.20.feed_forward.w3.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 192/ 237]            layers.20.ffn_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 193/ 237]        layers.21.attention.wq.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 194/ 237]        layers.21.attention.wk.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 195/ 237]        layers.21.attention.wv.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.076 0.056 0.039 0.025 0.021 \n",
            "[ 196/ 237]        layers.21.attention.wo.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 197/ 237]      layers.21.attention_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 198/ 237]     layers.21.feed_forward.w1.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.026 0.021 \n",
            "[ 199/ 237]     layers.21.feed_forward.w2.weight -     8640 x  3200, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 200/ 237]     layers.21.feed_forward.w3.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 201/ 237]            layers.21.ffn_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 202/ 237]        layers.22.attention.wq.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.097 0.111 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 203/ 237]        layers.22.attention.wk.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 204/ 237]        layers.22.attention.wv.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 205/ 237]        layers.22.attention.wo.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 206/ 237]      layers.22.attention_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 207/ 237]     layers.22.feed_forward.w1.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 208/ 237]     layers.22.feed_forward.w2.weight -     8640 x  3200, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 209/ 237]     layers.22.feed_forward.w3.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 210/ 237]            layers.22.ffn_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 211/ 237]        layers.23.attention.wq.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 212/ 237]        layers.23.attention.wk.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 213/ 237]        layers.23.attention.wv.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.076 0.096 0.111 0.118 0.112 0.096 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 214/ 237]        layers.23.attention.wo.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.037 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.116 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 215/ 237]      layers.23.attention_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 216/ 237]     layers.23.feed_forward.w1.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 217/ 237]     layers.23.feed_forward.w2.weight -     8640 x  3200, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 218/ 237]     layers.23.feed_forward.w3.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 219/ 237]            layers.23.ffn_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 220/ 237]        layers.24.attention.wq.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 221/ 237]        layers.24.attention.wk.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 222/ 237]        layers.24.attention.wv.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.118 0.111 0.096 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 223/ 237]        layers.24.attention.wo.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 224/ 237]      layers.24.attention_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 225/ 237]     layers.24.feed_forward.w1.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 226/ 237]     layers.24.feed_forward.w2.weight -     8640 x  3200, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.015 0.025 0.039 0.056 0.077 0.097 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 227/ 237]     layers.24.feed_forward.w3.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.037 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.116 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 228/ 237]            layers.24.ffn_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 229/ 237]        layers.25.attention.wq.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 230/ 237]        layers.25.attention.wk.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 231/ 237]        layers.25.attention.wv.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.096 0.112 0.118 0.112 0.097 0.077 0.056 0.039 0.025 0.021 \n",
            "[ 232/ 237]        layers.25.attention.wo.weight -     3200 x  3200, type =    f16, quantizing .. size =    19.53 MB ->     5.49 MB | hist: 0.036 0.015 0.025 0.038 0.056 0.076 0.097 0.112 0.118 0.112 0.097 0.076 0.056 0.038 0.025 0.021 \n",
            "[ 233/ 237]      layers.25.attention_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "[ 234/ 237]     layers.25.feed_forward.w1.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.097 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 235/ 237]     layers.25.feed_forward.w2.weight -     8640 x  3200, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.015 0.024 0.037 0.055 0.076 0.097 0.114 0.121 0.114 0.097 0.076 0.055 0.038 0.024 0.020 \n",
            "[ 236/ 237]     layers.25.feed_forward.w3.weight -     3200 x  8640, type =    f16, quantizing .. size =    52.73 MB ->    14.83 MB | hist: 0.036 0.016 0.025 0.039 0.057 0.077 0.096 0.111 0.117 0.111 0.097 0.077 0.057 0.039 0.025 0.021 \n",
            "[ 237/ 237]            layers.25.ffn_norm.weight -             3200, type =    f32, size =    0.012 MB\n",
            "llama_model_quantize_internal: model size  =  6535.80 MB\n",
            "llama_model_quantize_internal: quant size  =  1838.66 MB\n",
            "llama_model_quantize_internal: hist: 0.036 0.016 0.025 0.039 0.056 0.077 0.096 0.111 0.117 0.111 0.096 0.077 0.057 0.039 0.025 0.021 \n",
            "\n",
            "main: quantize time = 54386.68 ms\n",
            "main:    total time = 54386.68 ms\n",
            "main: build = 800 (481f793)\n",
            "main: seed  = 1688720397\n",
            "llama.cpp: loading model from ./models/open_llama_3b/ggml-model-q4_0.bin\n",
            "llama_model_load_internal: format     = ggjt v3 (latest)\n",
            "llama_model_load_internal: n_vocab    = 32000\n",
            "llama_model_load_internal: n_ctx      = 512\n",
            "llama_model_load_internal: n_embd     = 3200\n",
            "llama_model_load_internal: n_mult     = 240\n",
            "llama_model_load_internal: n_head     = 32\n",
            "llama_model_load_internal: n_layer    = 26\n",
            "llama_model_load_internal: n_rot      = 100\n",
            "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
            "llama_model_load_internal: n_ff       = 8640\n",
            "llama_model_load_internal: model size = 3B\n",
            "llama_model_load_internal: ggml ctx size =    0.06 MB\n",
            "llama_model_load_internal: mem required  = 2862.72 MB (+  682.00 MB per state)\n",
            "llama_new_context_with_model: kv self size  =  162.50 MB\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
            "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
            "generate: n_ctx = 512, n_batch = 512, n_predict = 128, n_keep = 0\n",
            "\n",
            "\n",
            " 8:55 am EDT, April 29, 2017\n",
            "‘The Handmaid’s Tale’ season 2 preview: What we know so far [Video]\n",
            "By Danielle Solzman\n",
            "By now we all know that The Handmaid’s Tale season 2 begins April 26 on Hulu.\n",
            "We also know what to expect this season from the show which is based on Margaret Atwood‘s classic novel. We also have seen that star Elisabeth Moss has been nominated for four Emmys for her work on the series and we know she will return in season 2.\n",
            "llama_print_timings:        load time =  5218.03 ms\n",
            "llama_print_timings:      sample time =   170.09 ms /   128 runs   (    1.33 ms per token,   752.56 tokens per second)\n",
            "llama_print_timings: prompt eval time =  1343.94 ms /     2 tokens (  671.97 ms per token,     1.49 tokens per second)\n",
            "llama_print_timings:        eval time = 51881.86 ms /   127 runs   (  408.52 ms per token,     2.45 tokens per second)\n",
            "llama_print_timings:       total time = 53439.97 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zyAHlkJVSBO",
        "outputId": "fff7bce5-0f02-49f1-838a-6b702bf228b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.1.68.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.6.3)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.24.0)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.68-cp310-cp310-linux_x86_64.whl size=265311 sha256=9dec9d8a0840800c7d1eca00d206a28856caae5c1a0c86671fc321f4e5c6dea7\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/f2/fb/b8153a244ace60fa4759cbd3d4881a2132b71e0e894ed6f29b\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.1 llama-cpp-python-0.1.68\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.225-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.16)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.9-py3-none-any.whl (26 kB)\n",
            "Collecting langchainplus-sdk<0.0.21,>=0.0.20 (from langchain)\n",
            "  Downloading langchainplus_sdk-0.0.20-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.24.0)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.9)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, marshmallow-enum, langchainplus-sdk, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.9 langchain-0.0.225 langchainplus-sdk-0.0.20 marshmallow-3.19.0 marshmallow-enum-1.5.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-cpp-python\n",
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZrSFQn3XywE"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.utilities import SerpAPIWrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPGvCxYTw7XY"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZfAx4oQw-IU"
      },
      "outputs": [],
      "source": [
        "# Callbacks support token-wise streaming\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "# Verbose is required to pass to the callback manager"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIc_3mhil6Gq",
        "outputId": "9f99b85e-94ef-4210-dbaf-a31ab9d194a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build-info.h\t\t ggml-metal.m\t   Package.swift\n",
            "build.zig\t\t ggml-metal.metal  perplexity\n",
            "CMakeLists.txt\t\t ggml.o\t\t   pocs\n",
            "common.o\t\t ggml-opencl.cpp   prompts\n",
            "convert-lora-to-ggml.py  ggml-opencl.h\t   quantize\n",
            "convert-pth-to-ggml.py\t k_quants.c\t   quantize-stats\n",
            "convert.py\t\t k_quants.h\t   README.md\n",
            "docs\t\t\t k_quants.o\t   requirements.txt\n",
            "embd-input-test\t\t libembdinput.so   scripts\n",
            "embedding\t\t LICENSE\t   server\n",
            "examples\t\t llama.cpp\t   SHA256SUMS\n",
            "flake.lock\t\t llama.h\t   simple\n",
            "flake.nix\t\t llama.o\t   spm-headers\n",
            "ggml.c\t\t\t llama-util.h\t   tests\n",
            "ggml-cuda.cu\t\t main\t\t   train-text-from-scratch\n",
            "ggml-cuda.h\t\t Makefile\t   vdot\n",
            "ggml.h\t\t\t media\n",
            "ggml-metal.h\t\t models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uLGWRPqxBbt",
        "outputId": "4bc7e3c0-b92a-42c9-a049-23af44172a54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ],
      "source": [
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"./models/open_llama_3b/ggml-model-q4_0.bin\", callback_manager=callback_manager, verbose=True\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPxLhWhZxD9G"
      },
      "outputs": [],
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Q2svclWxG-U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "0b16c645-d4a3-49f3-c4f6-d2bfd482113a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "*\n",
            "\n",
            "*Company name: First we choose a name for the company. A company name is\n",
            "the unique name of the organization, with no spaces between\n",
            "the words and no spaces after the last word. There are different rules\n",
            "for choosing a company name. The first is that the name must\n",
            "be in a language which is commonly understood around the world\n",
            "or at least commonly understood in your state or country. A second\n",
            "rule is that you cannot use the last name of an existing\n",
            "company that already exists. Let's take these rules and break them down.\n",
            "First, if the company name can be easily translated to English then it is in compliance with the first rule because it is a foreign language. It is also in compliance with the second rule if the company name is an existing company that does not already exist (other than as part of the name of another company). In this case, the company name is already being used.\n",
            "A company name can be made up or can be a combination of words. There are two approaches. Either it can be composed of only a single word and no spaces (like the words for example, as in my example above) or it can be composed of many different words and spaces between them ("
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n*\\n\\n*Company name: First we choose a name for the company. A company name is\\nthe unique name of the organization, with no spaces between\\nthe words and no spaces after the last word. There are different rules\\nfor choosing a company name. The first is that the name must\\nbe in a language which is commonly understood around the world\\nor at least commonly understood in your state or country. A second\\nrule is that you cannot use the last name of an existing\\ncompany that already exists. Let's take these rules and break them down.\\nFirst, if the company name can be easily translated to English then it is in compliance with the first rule because it is a foreign language. It is also in compliance with the second rule if the company name is an existing company that does not already exist (other than as part of the name of another company). In this case, the company name is already being used.\\nA company name can be made up or can be a combination of words. There are two approaches. Either it can be composed of only a single word and no spaces (like the words for example, as in my example above) or it can be composed of many different words and spaces between them (\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "question = \"What would be a good company name for a company that makes colorful socks\"\n",
        "\n",
        "llm_chain.run(question)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOhX9PSCbzglYRkgqIatmto"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}